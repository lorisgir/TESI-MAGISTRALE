\chapter{Conclusione}
\label{chap:conclusion}
L'obiettivo di questa tesi era quello di portare avanti la collaborazione al progetto Beat 4.0 tra SKF e ALTEN concentrandosi sull'Anomaly Detection per proporre uno strumento che aiutasse gli operatori all'interno dell'impianto di produzione. La scarsa frequenza di dati, una mappatura non completa dei macchinari e soprattutto la mancanza di etichette ha posto una sfida interessante nell'applicazione di un modello di Anomaly Detection che funzionasse bene.
Per queste motivazioni si è deciso di concentrarsi nello sviluppo di tecniche di Model Selection non supervisionato ma che fossero comparabili alle più classiche metriche supervisionate come F1-Score.
Sono state proposte tre metriche definite "surrogate": \textit{Model Centrality}, \textit{Clustering Coefficient} e \textit{Performance on Injected Synthetic Anomalies} insieme a delle valutazioni approfondite rispetto a dataset di benchmark provenienti da ODDS e SMD. Queste sono state fatte utilizzando dei coefficienti di correlazione paragonando le classifiche prodotte dal Model Selection rispetto alla classifica generata tramite etichette con F1-Score.

La lista di modelli candidati usata è stata resa varia sia per tipologia di Anomaly Detector che per complessità: non solo semplici modelli basati sulla distanza come KNN, ma anche modelli lineari, probabilistici o a rete neurale. Quest'ultimi a loro volta erano di differenti tipologie: reti neurali profonde feed-forward, reti ricorrenti con memoria, reti a grafo, auto encoder e reti generative avversarie.

La metrica surrogata che generalmente aveva performance migliori era Model Centrality, mentre Clustering Coefficient e Performance on Injected Synthetic Anomalies producevano risultati molto differenti in base al dataset analizzato. Comportamento confermato dal fatto che sono i due metodi che lavorano direttamente sul dataset andando o a effettuare clustering o andando a modificare i valori per iniettare anomalie, mentre Model Centrality lavora sugli output dei modelli e quindi riusciva a essere più stabile da questo punto di vista e non solo. Model Centrality, sfruttando e combinando le informazioni provenienti dagli output di ogni modello, riesce ad ottenere performance superiori; quindi riuscire a utilizzare ed aggregare una grande varietà di informazioni porta a dei miglioramenti in questo senso.
Nell'ottica di aggregazione delle metriche, invece, è quindi utile andare a valutare quale metrica surrogata utilizzare per ogni dataset in modo da produrre risultati migliori: combinare sempre e comunque tutte le metriche, indipendentemente dal dataset, può risultare controproducente.

All'interno del contesto SKF, l'algoritmo di Model Selection ha prodotto risultati soddisfacenti. Mentre è vero che l'analisi dei risultati paragonati con la qualità non ha dato conferme, un'analisi visiva dei dati attraverso grafici a linee o scatter ha permesso di avere un'idea delle performance del modello classificato come migliore dal Model Selection, sopratutto se paragonato rispetto al modello ultimo in classifica. 

Sviluppi futuri potrebbero andare ad approfondire tutta la parte di thresholding, ad esempio implementando un sistema di Model Selection che itera rispetto ad una lista di fattori di contaminazione passata come parametro per poi andare ad aggregare i risultati alla fine. Questo perché il fattore di contaminazione non è conosciuto e le tecniche che stimano questo valore potrebbero sbagliare anche in maniera significativa andando poi a falsare i risultati dell'algoritmo.
Un altro punto da poter approfondire riguarda la lista di modelli candidati passata all'algoritmo, così come i valori degli iperparametri di ogni modello. In questa tesi si è cercato di variare di molto le tipologie di rilevatori di anomalie, andando però a cambiare di poco gli iperparametri rispetto ai valori di default. Cambiarli potrebbe alterare le performance dei modelli e nel complesso anche i risultati del Model Selection.
Sarebbe anche interessante se in futuro i principali aspetti negativi del dataset di SKF venissero risolti per poter valutare in maniera più accurata i risultati del Model Selection rispetto ai dataset dei macchinari. Anche senza la presenza di etichette, che richiederebbe un costo di etichettatura non indifferente, riuscire anche solo ad alzare la frequenza di registrazione dei dati a 1/minuto porterebbe moltissimi benefici. Così come la mappatura completa del canale di produzione che consentirebbe forse di trovare una correlazione tra le anomalie e la qualità finale del prodotto, dando così un aiuto maggiore agli operatori nelle decisioni.