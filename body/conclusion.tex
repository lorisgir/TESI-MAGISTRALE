\chapter{Conclusione}
\label{chap:conclusion}
L'obiettivo di questa tesi era quello di portare avanti la collaborazione al progetto Beat4.0 tra SKF e Alten concentrandosi sull'Anomaly Detection per proporre uno strumento che aiutasse gli operatori all'interno dell'impianto di produzione. La scarsa frequenza di dati, una mappatura non completa dei macchinari e soprattutto la manca di label ha posto una sfida interessante nell'applicazione di un modello di Anomaly Detection che funzionasse bene.
Per queste motiviazione si é deciso di concentrarsi nello sviluppo di tecniche di Model Selection non supervisionato ma che fossero comparabili alle piu classiche metriche supervisionate come F1-Score.
Sono state proposte 3 metriche definite "surrogate": \textit{model centrality}, \textit{clustering coefficient} e \textit{performance on injected synthetic anomalies} insieme a delle valutazioni approfondite rispetto a dataset di benchmark provenienti da ODDS e SMD utilizzando coefficienti di correlazione rispetto alla ground-truth proveniente da F1-Score.
La lista di modelli candidati usata é stata resa varia sia per tipologia di Anomaly Detector che per complessita: non solo semplici modelli basati sulla distanza come k-NN, ma anche modelli lineari, probabilistici e a rete neurale. Quest'ultimi a loro volta possedevano strutture completamente differenti: reti neurali profonde feed-forward, reti ricorrenti con memoria, reti a grafo, auto encoder e reti generative avversarie.
La metrica surrogata che generalmente aveva performance migliori era Model Centrality, mentre Clustering Coefficien e Performance on Injected Sunthetic Anomalies producevano risultati molto differenti in base al dataset analizzato. Comportamento confermato dal fatto che sono i due metodi che lavorano direttamente sul dataset andando o a effettuare clustering o andando a modificare i valori per iniettare anomalie, mentre Model Centrality lavora sugli output dei modelli e quindi riesce a essere piu stabile da questo punto di vista.
é quindi necessario andare a valutare quale metrica surrogata utilizzare per ogni dataset in modo da produrre risultati migliori: combinare sempre e comunque tutte e tre le metriche, indipendentemente dal dataset, puo risultare controproducente.
All'interno del contesto SKF, l'algoritmo di Model Selection ha prodotto risultati soddisfacenti. Mentre é vero che l'analisi dei risultati paragonati con la qualita non ha dato conferme, un'analisi visiva dei dati attraverso grafici a linee o scatter ha permesso di avere un'idea delle performance del modello classificato come migliore dal Model Selection, sopratutto se paragato rispetto al modello ultimo in classifica.
Sviluppi futuri potrebbero andare ad approfondire il dicorso di thresholding, ad esempio implementando o un sistema di Model Selection che itera tutti i passaggi rispetto ad una lista di fattori di contaminazione passata come parametro per poi andare ad aggregare i risultati alla fine. Questo perche il fattore di contaminzione non é conosciuto e le tecniche che stimano questo valore potrebbero sbagliare anche in maniera significativa andando poi a falsare i risultati del model selection.
Un'altro punto da poter approfondire riguarda la lista di modelli candidati passata all'algoritmo, cosi come i valori degli iper-parametri di ogni modello. In questa tesi si é cercato di variare di molto le tipologie di rilevatori di anomalie, andando però a cambiare di poco gli iper-parametri rispetto ai valori di default. Cambiarli potrebbe alterare le performance dei modelli e nel complesso anche i risultati del model selection.
Sarebbe anche interessante se in futuro i principali aspetti negativi del dataset di SKF venissero risolti per poter valutare in maniera piu accurata i risultati del model selection rispetto ai dataset dei macchinari. Anche senza la presenza di label, che richiederebbe un costo di labellatura non indifferente, riuscire anche solo ad alzare la frequenza di registrazione dei dati a 1/minuto o addirituttura 1/10 secondi portebbe moltissimi benefici. Cosi come la mappatura completa del canale di produzione che consentirebbe forse di trovare una correlazione tra le anomalie e la qualita finale del prodotto.