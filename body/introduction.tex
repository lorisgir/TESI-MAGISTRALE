\chapter{Introduction}
\label{chap:first-chapter-intro}
Il campo dell'Anomaly Detection sta ricevendo molto interesse nell'ultimo periodo: l'avanzamento tecnologico ha portato alla nascita dei Big Data e degli approcci Data Driven consentendo a tecniche di machine learning di spopolare in tantissimi ambiti, tra cui quello dell'industria 4.0.
Attraverso l'innovazione degli impianti di produzione e all'utilizzo di dispositivi iot, gli operatori possono monitoare il comportamento di un canale di produzione e intervenire nel caso di anomalie oppure pianificare la produzione grazie all'analisi di dati statistici o report.
Solamente questo ha permesso un evoluzione sostanziale ma non ci si ferma qui, l'ernome quantita di dati raccolta rende possibile l'applicazione di tecniche di machine learning come supporto alle scelte decisionali in diversi ambiti: anomaly detection, preedictive maintenance, forecasting e molto altro.

L’obiettivo di questa tesi é quello di contribuire al progetto Beat 4.0 portato avanti da SKF e ALTEN ITALIA. SKF e' una azienda multinazionale specializzata nella produzione di cuscinetti a sfera, ALTEN e' una'azienda leader nel mercato europeo della consulenza IT e che sta offrendo supporto ad SKF. Questa collaborazione ha lo scopo di portare innovazione digitale all'impianto SKF di Cassino in provincia di Frosinone, attraverso l'applicazione di tecniche di Machine Learning.
Diverse collaborazioni sono state concluse, mentre altre ancora in corso, tra Alten e l'Universita degli Studi di Torino per portare avanti il progetto Beat4.0. Ognuna di queste si focalizzava in un dominio specifico del condition monitoring: forecasting della qualita', studio di correlazione tra i sensori e la qualita finale, preedictive maintenance ed infine anomaly detection su cui si basa il lavoro di questa tesi.
Il contributo dello studente si concentra su tutto il ciclo di vita di produzione di tecniche di machine learning: studio qualitativo e processamento dei dati, analisi e applicazione dei metodi di anomaly detection ed infine distribuzione degli algoritmi su Cloud Azure e sviluppo front-end attraverso PowerBI.
Il focus della tesi si e' concentrato prevalentemente sullo studio e sull'applicazione di tecniche di Model Selection non supervisionato: la mancanza di labels all'interno del dataset fornitorici da SKF non permette la valutazione di un modello di Anomaly Detection, costrigendo quindi a ricercare metodi che non ne richiedessero l'uso. Solitamente il Model Selection si effettua andando a partizionare i dati in modo diverso per la fase di train/test, oppure facendo ottimizzazione degli iper-parametri, ma in ogni caso la presenza di label e' necessaria. Per questo motivo vengono introdotte due tecniche di Model Selection non supervisionate, chiamate Meta Learning e Metriche Surrogate. La prima richiede di allenare dei rilevatori di anomalie su dataset labellati per poi trasferire le informazioni apprese ad un dataset non labellato che sia sufficientemente simile al primo, quindi questo approccio e' stato scartato. Il secondo si basa sulla creazione di metriche non supervisionate, chiamate surrogate, che siano sufficientemente correlate alle piu classiche metriche supervisionate come F1-Score. Verranno quindi proposte tre metriche surrogate chiamate \textit{model centrality}, \textit{clustering coefficient} e \textit{performance on injected synthetic anomalies} insieme a diversi metodi di Rank Aggregation: Borda, Robust Borda, AVG Score e Kemedy-Young utilizzati per combinare insieme le 3 metriche non supervisionate.

Il proseguo della tesi si divide quindi in questo modo: i \textit{Capitoli \ref{chap:intro}}, \textit{\ref{chap:methods}} e \textit{\ref{chap:modelselection}} presentano un'introduzione all'Anomaly Detection, metodi di Anomaly Detection e Tresholding, e Model Selection rispettivamente. Nel \textit{Capitolo \ref{chap:skf}} vengono trattati i dati SKF e le loro proprieta e carratteristiche. Il \textit{Capitolo \ref{chap:impl}} tratta tutta la fase di sperimentazione, con una prima parte dedicata all'implementazione delle metriche surrogate e di rank aggregation, una seconda parte contentende i risultati degli esperimenti su dataset di benchmark provenienti da ODDS e SMD ed infine una terza parte in cui viene testato l'algoritmo di model selection su SKF. Il \textit{Capitolo \ref{chap:deploy}} conclude il ciclo di vita del software andando a trattare la fase di pubblicazione su Cloud Azure.
Conclusione e sviluppi futuri sono invece trattati nel \textit{Capitolo \ref{chap:conclusion}}.